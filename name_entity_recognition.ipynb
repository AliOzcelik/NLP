{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/desidero/Desktop/Kodlar/NLP/NER/ner_dataset.csv\"\n",
    "model_name = 'bert-base-uncased'\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(object):\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path, encoding=\"unicode_escape\")\n",
    "        self.n_tags, self.n_pos, self.pos2ids, self.tags2ids, self.id2pos, self.id2tag = self.transform()\n",
    "        self.pre_sentences, self.pos, self.tags = self.organise()\n",
    "        self.sentences = self.clean()\n",
    "        #self.correction()\n",
    "    \n",
    "    def correction(self):\n",
    "        self.sentences[1901][0] = 'None'\n",
    "        self.sentences[1956][0] = 'None'\n",
    "        self.sentences = np.vectorize(lambda x: str(x))(self.sentences)\n",
    "\n",
    "    def transform(self):\n",
    "        tag_list = self.data['Tag'].unique()\n",
    "        pos_list = self.data['POS'].unique()\n",
    "        n_tags = len(tag_list)\n",
    "        n_pos = len(pos_list)\n",
    "\n",
    "        pos2ids = {pos: i+1 for i, pos in enumerate(pos_list)}\n",
    "        id2pos = {i+1: pos for i, pos in enumerate(pos_list)}\n",
    "        self.data[\"PosId\"] = self.data[\"POS\"].map(pos2ids)\n",
    "\n",
    "        tags2ids = {tag: i+1 for i, tag in enumerate(tag_list)}\n",
    "        id2tag = {i+1: tag for i, tag in enumerate(tag_list)}\n",
    "        self.data[\"TagId\"] = self.data[\"Tag\"].map(tags2ids)\n",
    "\n",
    "        return n_tags, n_pos, pos2ids, tags2ids, id2pos, id2tag\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"i'm\", \"i am\", text) # replace \"i'm\" with \"i am\"\n",
    "        text = re.sub(r\"im\", \"i am\", text)\n",
    "        text = re.sub(r\"ive\", \"i have\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"n't\", \"not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"wont\", \"will not\", text)\n",
    "        text = re.sub(r\"won t\", \"will not\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"didnt\", \"did not\", text)\n",
    "        text = re.sub(r\"didn t\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"cant\", \"cannot\", text)\n",
    "        text = re.sub(r\"can t\", \"cannot\", text)\n",
    "        text = re.sub(r\"[-()\\\"#/@:<>{}+=~|.?,!;]\", \"\", text)\n",
    "        return text \n",
    "\n",
    "    def clean(self):\n",
    "        clean_sentences = [[self.clean_text(i) for i in x if type(i) == str] for x in self.pre_sentences]\n",
    "        return clean_sentences\n",
    "\n",
    "    def organise(self):\n",
    "\n",
    "        self.data.rename(columns={'Sentence #':'Sentence'}, inplace=True)\n",
    "        self.data[\"Sentence\"] = self.data[\"Sentence\"].fillna(method='ffill')\n",
    "\n",
    "        sentences = self.data.groupby('Sentence')['Word'].apply(list).values\n",
    "        pos = self.data.groupby('Sentence')['PosId'].apply(list).values\n",
    "        tags = self.data.groupby('Sentence')['TagId'].apply(list).values\n",
    "\n",
    "        return sentences, pos, tags\n",
    "    \n",
    "pre = Preprocess(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'london',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'british',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, pos_id, tag_id, tokenizer, max_len=128):\n",
    "        super(NerDataset, self).__init__()\n",
    "        self.text = sentences\n",
    "        self.pos_id = pos_id\n",
    "        self.tag_id = tag_id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        text = self.text[x]\n",
    "        pos = self.pos_id[x]\n",
    "        tag = self.tag_id[x]\n",
    "        ids = []\n",
    "        attention_mask = []\n",
    "        token_type_ids = []\n",
    "\n",
    "        for i in text:\n",
    "            encoding = self.tokenizer(i, add_special_tokens=False)\n",
    "            ids.extend(encoding['input_ids'])\n",
    "            attention_mask.extend(encoding['attention_mask'])\n",
    "            token_type_ids.extend(encoding['token_type_ids'])\n",
    "\n",
    "        # [101]: CLS, [102]: SEP, [0]: PAD\n",
    "        ids = [101] + ids + [102]\n",
    "        pos = [0] + pos + [0]\n",
    "        tag = [0] + tag + [0]\n",
    "        pos = pos + [0]*(self.max_len - len(pos))\n",
    "        tag = tag + [0]*(self.max_len - len(tag))\n",
    "        pos = pos[:128]\n",
    "        tag = tag[:128]\n",
    "\n",
    "        ids = ids + [0]*(self.max_len - len(ids))\n",
    "        attention_mask = attention_mask + [0]*(self.max_len - len(attention_mask))\n",
    "        token_type_ids = token_type_ids + [0]*(self.max_len - len(token_type_ids))\n",
    "        \n",
    "        return {'input_ids': torch.tensor(ids, dtype=torch.long, device=device), \n",
    "                'pos': torch.tensor(pos, dtype=torch.long, device=device), \n",
    "                'tag': torch.tensor(tag, dtype=torch.long, device=device),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long, device=device), \n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long, device=device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence, valid_sentence, train_pos, valid_pos, train_tags, valid_tags = train_test_split(pre.sentences, pre.pos, pre.tags, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataset(train_sentence, train_pos, train_tags, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, drop_last=True)\n",
    "\n",
    "valid_dataset = NerDataset(valid_sentence, valid_pos, valid_tags, tokenizer)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i['input_ids'].size())\n",
    "    print(i['pos'].size())\n",
    "    print(i['tag'].size())\n",
    "    print(i['attention_mask'].size())\n",
    "    print(i['token_type_ids'].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert_model_name, n_pos, n_tags):\n",
    "        super(NerModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.pos_dropout = nn.Dropout(0.1)\n",
    "        self.tag_dropout = nn.Dropout(0.1)\n",
    "        self.fc_pos = nn.Linear(self.bert.config.hidden_size, n_pos)\n",
    "        self.fc_tag = nn.Linear(self.bert.config.hidden_size, n_tags)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_outputs = outputs.last_hidden_state\n",
    "\n",
    "        x_pos = self.pos_dropout(pooler_outputs)\n",
    "        x_tag = self.pos_dropout(pooler_outputs)\n",
    "\n",
    "        x_pos = self.fc_pos(x_pos)\n",
    "        x_tag = self.fc_tag(x_tag)\n",
    "\n",
    "        return x_pos, x_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "num_epochs = 5\n",
    "model = NerModel(model_name, pre.n_pos, pre.n_tags).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(prediction, target, masks, criterion, n_classes):\n",
    "    logits = prediction.view(-1, n_classes)\n",
    "    masks = masks.view(-1)\n",
    "    target = target.view(-1)\n",
    "    target = torch.where(masks == 1, target, torch.tensor(criterion.ignore_index).type_as(target))\n",
    "    return criterion(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pos_output, tag_output = model(input_ids = ['input_ids'], attention_mask = batch['attention_mask'], token_type_ids = batch['token_type_ids'])\n",
    "        pos_loss = loss_fn(pos_output, batch['pos'], batch['attention_mask'], criterion, pre.n_pos)\n",
    "        tag_loss = loss_fn(tag_output, batch['tag'], batch['attention_mask'], criterion, pre.n_tags)\n",
    "        loss = (pos_loss + tag_loss)/2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "    pos_loss_list = []\n",
    "    tag_loss_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            pos_output, tag_output = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'], token_type_ids = batch['token_type_ids'])\n",
    "            pos_loss = loss_fn(pos_output, batch['pos'], batch['attention_mask'], criterion, pre.n_pos)\n",
    "            tag_loss = loss_fn(tag_output, batch['tag'], batch['attention_mask'], criterion, pre.n_tags)\n",
    "            pos_loss_list(pos_loss)\n",
    "            tag_loss_list(tag_loss)\n",
    "\n",
    "    return sum(pos_loss_list)/len(pos_loss_list), sum(tag_loss_list)/len(tag_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train(model, train_dataloader, optimizer, scheduler, criterion)\n",
    "    pos_loss, tag_loss = evaluate(model, valid_dataloader)\n",
    "    print(\"POS: \", pos_loss)\n",
    "    print(\"TAG: \", tag_loss)\n",
    "    print(\"-----------------\")\n",
    "    toc = time.time()\n",
    "    print(\"epoch time: \", toc-tic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
