{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\"\n",
      " 'Hello, and thank you for your question and seeking advice on this. Feelings of worthlessness is unfortunately common. In fact, most people, if not all, have felt this to some degree at some point in their life. You are not alone.\\xa0Changing our feelings is like changing our thoughts - it\\'s hard to do. Our minds are so amazing that the minute you change your thought another one can be right there to take it\\'s place. Without your permission, another thought can just pop in there. The new thought may feel worse than the last one! My guess is that you have tried several things to improve this on your own even before reaching out on here. People often try thinking positive thoughts, debating with their thoughts, or simply telling themselves that they need to \"snap out of it\" - which is also a thought that carries some self-criticism.\\xa0Some people try a different approach, and there are counselors out there that can help you with this. The idea is that instead of trying to change the thoughts, you change how you respond to them. You learn skills that allow you to manage difficult thoughts and feelings differently so they don\\'t have the same impact on you that they do right now. For some people, they actually DO begin to experience less hurtful thoughts once they learn how to manage the ones they have differently. Acceptance and Commitment Therapy may be a good choice for you.\\xa0There is information online and even self-help books that you can use to teach you the skills that I mentioned. Because they are skills, they require practice, but many people have found great relief and an enriched life by learning them.\\xa0As for suicidal thoughts, I am very glad to read that this has not happened to you. Still, you should watch out for this because it can be a sign of a worsening depression. If you begin to think about this, it is important to reach out to a support system right away. The National Suicide Prevention Lifeline is 1-800-273-8255. The text line is #741741.\\xa0I hope some other colleagues will provide you more suggestions.\\xa0Be well...Robin Landwehr, DBH, LPCC']\n",
      "[\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\"\n",
      " \"First thing I'd suggest is getting the sleep you need or it will impact how you think and feel. I'd look at finding what is going well in your life and what you can be grateful for. I believe everyone has talents and wants to find their purpose in life. I think you can figure it out with some help.\"]\n",
      "3512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  I'm going through some things with my feelings...   \n",
       "1  I'm going through some things with my feelings...   \n",
       "2  I'm going through some things with my feelings...   \n",
       "3  I'm going through some things with my feelings...   \n",
       "4  I'm going through some things with my feelings...   \n",
       "\n",
       "                                            Response  \n",
       "0  If everyone thinks you're worthless, then mayb...  \n",
       "1  Hello, and thank you for your question and see...  \n",
       "2  First thing I'd suggest is getting the sleep y...  \n",
       "3  Therapy is essential for those that are feelin...  \n",
       "4  I first want to let you know that you are not ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/desidero/Desktop/Kodlar/NLP/Psychology/train.csv')\n",
    "print(dataset.iloc[1,:].values)\n",
    "print(dataset.iloc[2,:].values)\n",
    "print(len(dataset))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am going through some things with my feelings and myself i barely sleep and i do nothing but think about how i am worthless and how i shouldnot be here\n",
      "   i have never tried or contemplated suicide i have always wanted to fix my issues but i never get around to it\n",
      "   how can i change my feeling of being worthless to everyone\n",
      "hello and thank you for your question and seeking advice on this feelings of worthlessness is unfortunately common in fact most people if not all have felt this to some degree at some point in their life you are not alone changing our feelings is like changing our thoughts  it's hard to do our minds are so amazing that the minute you change your thought another one can be right there to take it's place without your permission another thought can just pop in there the new thought may feel worse than the last one my guess is that you have tried several things to i amprove this on your own even before reaching out on here people often try thinking positi have thoughts debating with their thoughts or si amply telling themselves that they need to snap out of it  which is also a thought that carries some selfcriticism some people try a different approach and there are counselors out there that can help you with this the idea is that instead of trying to change the thoughts you change how you respond to them you learn skills that allow you to manage difficult thoughts and feelings differently so they donot have the same i ampact on you that they do right now for some people they actually do begin to experience less hurtful thoughts once they learn how to manage the ones they have differently acceptance and commitment therapy may be a good choice for you there is information online and even selfhelp books that you can use to teach you the skills that i mentioned because they are skills they require practice but many people have found great relief and an enriched life by learning them as for suicidal thoughts i am very glad to read that this has not happened to you still you should watch out for this because it can be a sign of a worsening depression if you begin to think about this it is i amportant to reach out to a support system right away the national suicide prevention lifeline is 18002738255 the text line is 741741 i hope some other colleagues will provide you more suggestions be wellrobin landwehr dbh lpcc\n"
     ]
    }
   ],
   "source": [
    "class Preprocess():\n",
    "\n",
    "    def __init__(self, df_path):\n",
    "        self.data = self.drop(pd.read_csv(df_path))\n",
    "        self.context = [self.clean_text(i) for i in self.data.iloc[:, 0].values]\n",
    "        self.response = [self.clean_text(i) for i in self.data.iloc[:, 1].values]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def drop(self, df):\n",
    "        return df.dropna(axis=0)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"i'm\", \"i am\", text) # replace \"i'm\" with \"i am\"\n",
    "        text = re.sub(r\"im\", \"i am\", text)\n",
    "        text = re.sub(r\"ive\", \"i have\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"n't\", \"not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"wont\", \"will not\", text)\n",
    "        text = re.sub(r\"won t\", \"will not\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"didnt\", \"did not\", text)\n",
    "        text = re.sub(r\"didn t\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"cant\", \"cannot\", text)\n",
    "        text = re.sub(r\"can t\", \"cannot\", text)\n",
    "        #text = re.sub(r\"[-()\\\"#/@:<>{}+=~|.?,!]\", \"\", text)\n",
    "        text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,!,...]\", \"\", text)\n",
    "        return text \n",
    "    \n",
    "path = '/Users/desidero/Desktop/Kodlar/NLP/Psychology/train.csv'\n",
    "pre = Preprocess(path)\n",
    "print(pre.context[1])\n",
    "print(pre.response[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PsychologyDataset():\n",
    "    \n",
    "    def __init__(self, path, tokenizer, max_length):\n",
    "        self.pre = Preprocess(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pre.context)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.pre.context[idx]\n",
    "        answer = self.pre.response[idx]\n",
    "        #encoding = self.tokenizer(text, answer, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')   \n",
    "\n",
    "        input_encoding = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        input_ids = input_encoding.input_ids.flatten()\n",
    "        attention_mask = input_encoding.attention_mask.flatten()\n",
    "        \n",
    "        # Tokenize output answer\n",
    "        output_encoding = self.tokenizer(answer, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        output = output_encoding.input_ids.flatten()\n",
    "        output_attention_mask = output_encoding.attention_mask.flatten()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'outputs': output,\n",
    "            'output_attention_mask': output_attention_mask \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "tokenizer = T.AutoTokenizer.from_pretrained('t5-small')\n",
    "data = PsychologyDataset(path, tokenizer, max_length=512)\n",
    "dataloader = DataLoader(data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n",
      "179\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(int(tokenizer.vocab_size ** 0.5))\n",
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_length=512)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=8, num_encoder_layers=6,\n",
    "                                           num_decoder_layers=6, dim_feedforward=512)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc_out.bias.data.zero_()\n",
    "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        \n",
    "        tgt_embedded = self.embedding(tgt) * math.sqrt(self.embedding_dim)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "        output = self.transformer(src_embedded, tgt_embedded)\n",
    "        output = self.fc_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/desidero/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "#vocab_size = tokenizer.vocab_size\n",
    "vocab_size = 512\n",
    "embedding_dim = 80\n",
    "model = TransformerModel(vocab_size, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        response = batch['outputs'].to(device)\n",
    "        #response_attention_mask = torch.tensor(batch['output_attention_mask']).to(device)\n",
    "        #print('input_ids: ', input_ids.shape)\n",
    "        #print('attention_mask: ', attention_mask.shape)\n",
    "        #print('response: ', response.shape)\n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = criterion(output, response).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train(dataloader, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: [[('? to indX', [0.01424487866461277, 0.012225417420268059, 0.011082618497312069, 0.010660824365913868, 0.009663427248597145]), ('? in to (d', [0.014583855867385864, 0.013112141750752926, 0.01108396053314209, 0.009704717434942722, 0.009258395060896873]), ('?d inX to', [0.016232678666710854, 0.01329257432371378, 0.01320330798625946, 0.010378205217421055, 0.009773727506399155]), ('? ind toX', [0.016989346593618393, 0.01225390937179327, 0.011101714335381985, 0.010956604033708572, 0.009675339795649052]), ('? ind to’', [0.017202647402882576, 0.01213556807488203, 0.01167673896998167, 0.010288580320775509, 0.009299002587795258])]]\n",
      "Bot: [[('? ind’l', [0.014129247516393661, 0.011437207460403442, 0.009581104852259159, 0.009494545869529247, 0.008833914063870907]), ('? inld for', [0.014958547428250313, 0.01170134823769331, 0.010144821368157864, 0.009437772445380688, 0.00917124468833208]), ('‘K todayele free', [0.007457095663994551, 0.00733140716329217, 0.006259845104068518, 0.00582541711628437, 0.005498555023223162]), ('? in todX', [0.016522392630577087, 0.012661309912800789, 0.011818614788353443, 0.011735303327441216, 0.010420157574117184]), ('? to ind (', [0.016176437959074974, 0.013284029439091682, 0.011129663325846195, 0.010282356292009354, 0.009634262882173061]), ('? ind to’', [0.017202647402882576, 0.01213556807488203, 0.01167673896998167, 0.010288580320775509, 0.009299002587795258])]]\n",
      "Bot: [[('?d inXa', [0.014170719310641289, 0.013044344261288643, 0.011869983747601509, 0.009569969028234482, 0.00914701446890831]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('? indX to', [0.016747666522860527, 0.013068469241261482, 0.012197945266962051, 0.011101446114480495, 0.009781520813703537]), ('? indX (', [0.016216861084103584, 0.012676850892603397, 0.012201336212456226, 0.009667706675827503, 0.009486766532063484]), ('? to inXd', [0.013323504477739334, 0.012452658265829086, 0.011084787547588348, 0.010269725695252419, 0.00978041160851717]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('‘K todayele free', [0.007457100786268711, 0.007331408094614744, 0.006259843707084656, 0.005825425498187542, 0.005498554091900587]), ('? in todX', [0.016522390767931938, 0.01266130618751049, 0.011818614788353443, 0.011735303327441216, 0.010420157574117184]), ('?d for in’', [0.011138864792883396, 0.009539677761495113, 0.009308548644185066, 0.008482737466692924, 0.007735839579254389]), ('?d in wea', [0.018463924527168274, 0.013003177009522915, 0.012936939485371113, 0.009490751661360264, 0.009236241690814495]), ('?d in toX', [0.016641149297356606, 0.01203949935734272, 0.01126755028963089, 0.010296111926436424, 0.010027537122368813]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('?d inr’', [0.014965257607400417, 0.01172096747905016, 0.01148287858814001, 0.00959465466439724, 0.009477484971284866]), ('‘K todayele free', [0.007457100786268711, 0.007331408094614744, 0.006259843707084656, 0.005825425498187542, 0.005498554091900587]), ('? in todX', [0.016522390767931938, 0.01266130618751049, 0.011818614788353443, 0.011735303327441216, 0.010420157574117184]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('? ind toX', [0.016989346593618393, 0.012253910303115845, 0.011101716198027134, 0.010956604033708572, 0.009675338864326477]), ('? ind to’', [0.01720264181494713, 0.01213556807488203, 0.01167673896998167, 0.010288580320775509, 0.009299003519117832])]]\n"
     ]
    }
   ],
   "source": [
    "def get_attention_mask(input_ids):\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    attention_mask[input_ids == 0] = 0\n",
    "    return attention_mask\n",
    "\n",
    "def decode_output(output_tensor, tokenizer, top_k=1):\n",
    "\n",
    "    output_probs = torch.softmax(output_tensor, dim=-1)\n",
    "    topk_probs, topk_ids = torch.topk(output_probs, top_k, dim=-1)\n",
    "    topk_probs = topk_probs.tolist()\n",
    "    topk_ids = topk_ids.tolist()\n",
    "\n",
    "    decoded_outputs = []\n",
    "\n",
    "    # Decode each top-k token ID sequence\n",
    "    for i in range(len(topk_ids)):\n",
    "        decoded_tokens = []\n",
    "        for j in range(len(topk_ids[i])):\n",
    "            # Decode token ID to text\n",
    "            decoded_token = tokenizer.decode(topk_ids[i][j])\n",
    "            decoded_tokens.append((decoded_token, topk_probs[i][j]))\n",
    "        decoded_outputs.append(decoded_tokens)\n",
    "\n",
    "    return decoded_outputs\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    model.eval()\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input == 'q':\n",
    "        break\n",
    "    \n",
    "    input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = get_attention_mask(input_ids)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    #print(output)\n",
    "    output_arr = output.cpu().numpy()\n",
    "    # Decode and print response\n",
    "    #print(output_arr)\n",
    "    #print(output_arr[0])\n",
    "    response = decode_output(output, tokenizer, top_k=5)\n",
    "    print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01280238  0.30230838  0.05524913  0.44777182  1.1704538   0.7730081\n",
      "  0.5674996   0.7819571   0.02924786  1.1252514   0.55719393 -0.29350805\n",
      "  1.0554311   0.1838808   0.60242146  0.580642    1.3858367   0.76135635\n",
      "  0.99606776  0.80064857  0.40890086  1.0350013   1.0540152   0.73256963\n",
      "  0.8335948   0.39472473  1.4801785   0.6284341   0.7718112   0.41725338\n",
      "  0.08415256  0.49672136  0.9954662   0.6130145   0.12179126  0.75483775\n",
      "  0.19233628  0.81176895  0.32335195  0.3762112   1.0719892   1.0783821\n",
      "  0.15511681  0.06561427  0.6832792   0.42233047 -0.1137569   0.80704194\n",
      " -0.19500998 -0.03225546  0.83643574  0.1820846   1.1188061   0.15590277\n",
      " -0.10242783  0.09076809 -0.5321743   0.00586918  1.5630016   0.24081416\n",
      "  0.10386963  0.18305212  0.88215286 -0.1388364   0.3993758  -0.03833205\n",
      "  0.64150435  0.418624    0.14951214  0.53025967  0.3857232   0.22231461\n",
      "  0.4586855   0.57853955 -0.06118084  0.29849523 -0.32207653 -0.13321657\n",
      " -0.36535296 -0.06709275 -0.09200664 -0.15544777  0.0364205  -0.00910634\n",
      "  0.11436068 -0.05867453 -0.3094164   0.04872322 -0.05971749 -0.22659546\n",
      "  0.21280612 -0.11039139  0.03891013 -0.21018639 -1.174388    0.4968412\n",
      " -0.68552125 -0.38794798 -0.7976561  -0.1749925  -0.8295682  -0.0851184\n",
      " -0.09986015  0.45477393 -0.38975233  0.06674656  0.56775355  0.165245\n",
      "  0.38510388 -0.49713436 -0.09716152  0.33415538 -0.41913018 -0.43765736\n",
      " -0.74809283 -0.9872077   0.24344482 -1.172506   -0.29278493 -0.54219425\n",
      " -0.24483833 -0.89834636 -0.6406497  -0.78095067 -0.9494805  -0.802951\n",
      "  0.1356084  -1.035761   -0.6733513  -0.09557166  0.32590762 -0.28577375\n",
      " -0.3891602  -0.5891701  -0.8806656  -0.67805046  0.4294135  -0.51299375\n",
      " -1.027532    0.04433215 -0.24721515 -0.31953257 -1.0718324   0.45552468\n",
      " -1.0171788  -1.1545775  -0.6137244  -0.33302936 -0.88584054 -0.4707643\n",
      " -0.44901767 -0.73494005 -0.93326485 -0.5211659  -0.5832176  -0.72562516\n",
      " -0.4312917  -0.66770643 -0.7315916  -1.3612818  -0.512738   -0.35441825\n",
      " -0.71725243 -0.5737467  -0.8466736  -1.3606458  -1.0952045  -1.2272481\n",
      " -0.6925216  -0.1574847  -0.01102273 -0.6466362  -1.5716429  -0.51251864\n",
      " -0.83664083 -1.1160327  -1.1335864  -0.6830968  -1.136653   -0.66665226\n",
      " -0.07600888 -2.3250525  -0.20223658 -0.66024595 -1.1653636  -1.1591773\n",
      " -0.52614665 -0.3996186  -1.3065732  -1.1334963  -0.830578   -0.49736404\n",
      " -1.3186277  -1.2233032  -1.3159599  -0.42295292 -0.8230699  -1.3407838\n",
      " -1.044069   -0.04525355 -1.3012964  -0.33183366 -1.612103   -0.872415\n",
      " -0.7514352  -1.521342   -0.6907739  -1.0135723  -0.08625732 -2.1911533\n",
      " -0.39141816 -1.1516339  -0.5823226  -0.96537256 -0.52392167 -0.09583569\n",
      " -0.9536792  -0.2813375  -1.3665361  -1.3089246  -0.95984834 -0.795408\n",
      " -0.73543537 -0.83338    -0.3709259  -0.98149705 -1.6608973  -1.0489862\n",
      " -0.6377656  -0.9495076  -0.7776002  -1.781192   -1.669593   -0.81535345\n",
      " -1.904304   -0.57833123 -0.44266292 -1.5946428  -0.9454934  -1.6985129\n",
      " -1.2701716  -0.67086905 -1.4995157  -0.69148475 -0.7513236  -1.1081872\n",
      " -1.2669456  -1.7312644  -0.42623466 -0.7721938  -0.08488282 -0.04479885\n",
      " -0.57280666 -1.0259802  -0.82954264 -0.56340086 -0.35444677 -0.45032093\n",
      " -0.7509302  -0.8527862  -1.1365619  -1.0037807  -1.4336747  -1.0119766\n",
      " -1.0489002  -0.7360594  -0.8359477  -1.0542928  -1.6179769  -1.5886065\n",
      " -1.1043897  -0.9457176  -0.22885211 -1.2858927  -1.2170384  -0.6562592\n",
      " -1.2814004  -0.6143347  -1.1571352  -1.2973566  -0.7470168  -1.524204\n",
      " -1.5005318  -0.6654659  -0.77619123 -0.42115408 -1.4334419  -1.513134\n",
      " -0.89238566 -0.8681121  -0.848503   -1.0209415  -1.5281794  -0.67089164\n",
      " -0.89972216 -1.6842121  -0.5167712  -1.0994394  -2.2950075  -0.8633132\n",
      " -1.182941   -1.0777888  -1.005614   -1.1304426  -1.5630076  -0.28392252\n",
      " -1.4865222  -1.2183391  -1.0299349  -0.4282329  -1.9854318  -1.077154\n",
      " -1.3027375  -0.36925521 -1.6108409  -0.12653568 -1.645272   -0.6742015\n",
      " -1.8371582  -1.4179875  -1.609524   -1.4162511   0.06786539 -1.7412397\n",
      " -1.1934628  -1.1561495  -0.9265339  -0.9289217  -0.873295   -0.3883517\n",
      " -1.2423036  -1.2038882  -1.1324466  -1.193842   -1.4671186  -1.234648\n",
      " -1.6119795  -1.6814631  -0.23684497 -1.399095   -1.0404919  -1.3914324\n",
      " -1.0722039  -0.47551778 -1.1018535  -1.0031058  -1.4922819  -0.9218348\n",
      " -1.2993492  -1.4342102  -0.9446762  -1.515639   -0.7459041  -1.0916662\n",
      " -1.4250509  -1.6418606  -1.4880224  -0.6031481  -1.0550275  -0.9641643\n",
      " -0.91695356 -0.9285281  -0.72783685 -1.3579592  -1.3841143  -1.0838978\n",
      " -1.0085988  -2.0554788  -1.1113085  -0.9309793  -1.3708013  -0.869157\n",
      " -0.8216157  -0.3652104  -0.8304485  -0.60547507 -1.998599   -0.9843261\n",
      " -1.1049271  -1.2833635  -0.6855572  -1.0689235  -0.64072096 -0.9158193\n",
      " -0.75425667 -0.87598073 -0.7230162  -1.4252704  -2.0570505  -1.3365587\n",
      " -0.8413882  -1.1527781  -1.1280427  -1.2618314  -1.5991787  -0.848788\n",
      " -0.15027063 -1.2110697  -1.4017738  -0.05824218 -1.8792484  -1.3694243\n",
      " -1.5536536  -1.2320236  -0.52571416 -1.3951311  -1.8700329  -1.5936475\n",
      " -0.86179775 -1.8113393  -1.4007833  -0.71767753 -0.8828855  -1.6435112\n",
      " -1.1006494  -1.9550023  -1.1057408  -1.7558274  -1.0341355  -0.4053276\n",
      " -2.0229065  -0.22792035 -0.30466205 -1.3462579  -2.7299294  -1.0545244\n",
      " -1.5401604  -0.40499616 -1.6982303  -0.5560519  -0.7507507  -0.80794764\n",
      " -1.7548344  -0.8479503  -1.2957973  -0.2453551  -1.2663761  -1.3830427\n",
      " -1.2077157  -2.207137   -0.15291876 -0.70166427 -1.8765528  -1.5761136\n",
      " -1.7019346  -1.6044911  -1.7835226  -1.6815841  -0.95834976 -0.69224894\n",
      " -0.4142775  -1.6528912  -1.0479504  -0.86059487 -1.2681723  -1.0339622\n",
      " -1.3234055  -1.4851259  -2.1359026  -1.1042554  -1.4364331  -0.06573731\n",
      " -0.9830169  -1.3761079  -1.5639489  -0.98848087 -0.7656003  -1.1440039\n",
      " -0.80439615 -1.8021493  -1.2206233  -2.2277267  -0.74379146 -1.4906614\n",
      " -1.0668747  -1.1535288  -0.6928543  -1.130663   -1.1566162  -1.1422336\n",
      " -0.8215915  -1.4005376  -0.32104656 -0.96133786 -1.6394216  -2.4404905\n",
      " -0.8178686  -1.0717798  -2.370394   -0.6634529  -0.61186093 -1.3001351\n",
      " -0.95119464 -0.1890285  -1.3771497  -0.95782506 -1.251072   -2.002783\n",
      " -1.4544463  -0.9769832  -0.8077931  -1.6081372  -1.6385655  -0.5246117\n",
      " -0.74594134 -1.1197578  -0.65906584 -1.7734418  -0.37423012 -0.48403388\n",
      " -1.2566789  -1.913383  ]\n"
     ]
    }
   ],
   "source": [
    "print(output_arr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1076, 1]\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer.encode(\"men\")\n",
    "print(a)\n",
    "b = tokenizer.decode(1076)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
